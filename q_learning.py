# -*- coding: utf-8 -*-
"""Q-learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IsYk8R77nAWARHkQqlRNOTeTikA1mdLi
"""

# Google Drive Mount

from google.colab import drive
drive.mount('/gdrive', force_remount=True)

# Data Preprocess
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

direction = {0: 'left', 1: 'up', 2: 'right', 3: 'down'}
left, up, right, down, dir = 0, 1, 2, 3, 4

alpha = 0.7 # Q-update present & future
gamma = 0.5 # Discount Factor
epsilon = 0.75
reward = 1
hole = -1
condition = False

f = "FrozenLake_3.txt"
fw = "FrozenLake_3_output.txt"
df = pd.read_csv("/gdrive/My Drive/dataset/" + f, sep = " ", header=None)

l = []
start = []
goal = []
num, rows, cols = int(df[0][0]), int(df[1][0]), int(df[2][0])
epi = rows*cols*1000
q_state = np.zeros((dir, rows, cols))

for i in range(rows):
  l.append(list(df[0][i+1].strip(' ')))
  for j in range(cols):
    if l[i][j] == 'S':
      start.append([i,j])
    elif l[i][j] == 'G':
      goal.append([i,j])

grid = np.array(l)

print("start: ", start)
print("goal: ", goal)
print(grid)
print(q_state.shape)

# greedy action: readq(cur) > choice maximum > action > readmaxq>update
# normal action: random choice > action > update
# update : readq(cur)
# Q(St,at) = (1-a)Q(St,at) + a*(Rt + g*Q_max(St+1,at+1))

class man:
  
  def __init__(self,q_state):
    self.y = start[0][0]  # row
    self.x = start[0][1]  # col
    self.py = start[0][0]
    self.px = start[0][1]
    self.q = np.zeros((dir, rows, cols))
    self.choice = 10
    self.episode = 0
    self.greedy = 1
    self.order = 1
    self.final = np.zeros((rows,cols), str)
    self.path =[]
  
  def Action(self, choice):
    while((abs(self.choice-choice)==2)):
      choice = np.random.randint(0,4)
    self.choice = choice

    if self.choice == left:
      if(self.x-1 >= 0):
        if([self.y,self.x-1] in self.path):
          return self.Action(np.random.randint(0,4))
        self.py = self.y
        self.px = self.x
        self.x -= 1
        self.path.append([self.py,self.px])
      else:
        self.q[self.choice][self.y][self.x] = -1
        return self.Action(np.random.randint(0,4))
    elif self.choice == up:
      if(self.y-1 >= 0):
        if([self.y-1,self.x] in self.path):
          return self.Action(np.random.randint(0,4))
        self.py = self.y
        self.px = self.x
        self.y -= 1
        self.path.append([self.py,self.px])
      else:
        self.q[self.choice][self.y][self.x] = -1
        return self.Action(np.random.randint(0,4))
    elif self.choice == right:
      if([self.y,self.x+1] in self.path):
        return self.Action(np.random.randint(0,4))
      if(self.x+1 < cols):
        if([self.y,self.x+1] in self.path):
          return self.Action(np.random.randint(0,4))
        self.py = self.y
        self.px = self.x
        self.x += 1
        self.path.append([self.py,self.px])
      else:
        self.q[self.choice][self.y][self.x] = -1
        return self.Action(np.random.randint(0,4))
    elif self.choice == down:
      if(self.y+1 < rows):
        if([self.y+1,self.x] in self.path):
          return self.Action(np.random.randint(0,4))
        self.py = self.y
        self.px = self.x
        self.y += 1
        self.path.append([self.py,self.px])
      else:
        self.q[self.choice][self.y][self.x] = -1
        return self.Action(np.random.randint(0,4))

  def ActionPath(self):
    if self.choice == left:
      if(self.x-1 >= 0):
        self.py = self.y
        self.px = self.x
        self.x -= 1
        self.path.append([self.py,self.px])
    elif self.choice == up:
      if(self.y-1 >= 0):
        self.py = self.y
        self.px = self.x
        self.y -= 1
    elif self.choice == right:
      if(self.x+1 < cols):
        self.py = self.y
        self.px = self.x
        self.x += 1
    elif self.choice == down:
      if(self.y+1 < rows):
        self.py = self.y
        self.px = self.x
        self.y += 1

  def ReadMaxQ(self):
    q_list = [self.q[left][self.y][self.x], self.q[up][self.y][self.x], self.q[right][self.y][self.x], self.q[down][self.y][self.x]]
    return max(q_list)

  def UpdateQ(self, grid):
    if(grid[self.y][self.x]=='H'):
      self.q[self.choice][self.py][self.px] = ((1-alpha)*self.q[self.choice][self.py][self.px]) + (alpha*(hole + gamma*self.ReadMaxQ()))
      self.y = self.py = start[0][0]
      self.x = self.px = start[0][1]
      self.path.clear()
      self.episode += 1
    elif(grid[self.y][self.x]=='G'):
      self.q[self.choice][self.py][self.px] = ((1-alpha)*self.q[self.choice][self.py][self.px]) + (alpha*(reward + gamma*self.ReadMaxQ()))      
      self.y = self.py = start[0][0]
      self.x = self.px = start[0][1]
      self.path.clear()
      self.episode += 1
    elif(grid[self.y][self.x]=='F'):
      self.q[self.choice][self.py][self.px] = ((1-alpha)*self.q[self.choice][self.py][self.px]) + (alpha*(0 + gamma*self.ReadMaxQ()))
    else:
      self.q[self.choice][self.py][self.px] = -1
    print(self.q)

  def UpdateGreedyQ(self, grid):
    if(grid[self.y][self.x]=='H'):
      self.q[self.choice][self.py][self.px] = ((1-alpha)*self.q[self.choice][self.py][self.px]) + (alpha*(hole + gamma*self.ReadMaxQ()))
      self.y = self.py = start[0][0]
      self.x = self.px = start[0][1]
      self.path.clear()
    elif(grid[self.y][self.x]=='G'):
      self.q[self.choice][self.py][self.px] = ((1-alpha)*self.q[self.choice][self.py][self.px]) + (alpha*(reward + gamma*self.ReadMaxQ()))      
      self.y = self.py = start[0][0]
      self.x = self.px = start[0][1]
      self.path.clear()
    else:
      self.q[self.choice][self.py][self.px] = ((1-alpha)*self.q[self.choice][self.py][self.px]) + (alpha*(0 + gamma*self.ReadMaxQ()))
    self.greedy += 1
    print(self.q)
  
  def GreedyQ(self):
    q_list = [self.q[left][self.py][self.px], self.q[up][self.py][self.px], self.q[right][self.py][self.px], self.q[down][self.py][self.px]]
    self.x = self.px
    self.y = self.py
    self.choice = q_list.index(max(q_list))
    self.Action(self.choice)

  def FindPath(self, grid):
    self.py = self.y = start[0][0]
    self.px = self.x = start[0][1]
    self.final[start[0][0]][start[0][1]] = "S"
    q_list = [self.q[left][self.y][self.x], self.q[up][self.y][self.x], self.q[right][self.y][self.x], self.q[down][self.y][self.x]]
    self.choice = q_list.index(max(q_list))
    self.ActionPath()
    print("=======================================")
    while(grid[self.y][self.x]!='G'):
      self.final[self.y][self.x] = "R"
      self.order += 1
      q_list = [self.q[left][self.y][self.x], self.q[up][self.y][self.x], self.q[right][self.y][self.x], self.q[down][self.y][self.x]]
      self.choice = q_list.index(max(q_list))
      self.ActionPath()
      print("=======================================")
    self.final[self.y][self.x] = 'G'

a = man(q_state)
while(a.episode != epi):
  a.Action(np.random.randint(0,4))
  a.UpdateQ(grid)
  if(a.episode == 1000):
    for _ in range(1,100):
      a.GreedyQ()
      a.UpdateGreedyQ(grid)  
  if(a.episode == 10000):
    for _ in range(1,400):
      a.GreedyQ()
      a.UpdateGreedyQ(grid)
  if(a.episode == 20000):
    for _ in range(1,1000):
      a.GreedyQ()
      a.UpdateGreedyQ(grid)
  print("episode: ", a.episode)

a.FindPath(grid)
print(a.final)
print(grid)
dfw = pd.DataFrame(a.final)
print(dfw)
dfw.to_csv("/gdrive/My Drive/dataset/" + fw, sep = " ", header=None)

